{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af901c3b-b52b-4739-aee4-008b11fec6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "import sys\n",
    "sys.path.append('../..'); sys.path.append('../');\n",
    "\n",
    "class HDSymbolicAttention(Layer):\n",
    "    def __init__(self, d_model, **kwargs):\n",
    "        super(HDSymbolicAttention, self).__init__(**kwargs)\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        a = input_shape[0][0]\n",
    "        self.value_weight = self.add_weight(name='value_weight',\n",
    "                                            shape=(64, self.d_model),\n",
    "                                            initializer='glorot_uniform',\n",
    "                                            trainable=True)\n",
    "        self.symbols = self.add_weight(shape=(3, 64),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.bn = layers.BatchNormalization(synchronized=True)\n",
    "        super(HDSymbolicAttention, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def cosine_similarity(self, a, b):\n",
    "        # Compute the cosine similarity as dot product divided by magnitudes\n",
    "        dot_product = tf.reduce_sum(tf.math.sign(a) * tf.math.sign(b), axis=-1)/1000\n",
    "        return dot_product   \n",
    "    \n",
    "    \n",
    "    def create_cosine_similarity_matrix(self,X):\n",
    "        X_expanded = tf.expand_dims(X, 2)  # Shape: (batch_size, N, 1, D)\n",
    "        X_repeated = tf.repeat(X_expanded, repeats=tf.shape(X)[1], axis=2)  # Shape: (batch_size, N, N, D)\n",
    "    \n",
    "        X_i_expanded = tf.expand_dims(X, 1)  # Shape: (batch_size, 1, N, D)\n",
    "        X_i_repeated = tf.repeat(X_i_expanded, repeats=tf.shape(X)[1], axis=1)  # Shape: (batch_size, N, N, D)\n",
    "    \n",
    "        X_i_plus_X_j = X_i_repeated + X_repeated  # Broadcasting adds the matrices element-wise\n",
    "    \n",
    "        S = self.cosine_similarity(X_i_repeated, X_i_plus_X_j)  # Shape: (batch_size, N, N)\n",
    "    \n",
    "        return tf.nn.softmax(S)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Unpack the inputs (queries, keys, values)\n",
    "        queries, keys, values = inputs\n",
    "\n",
    "        # Linear projections\n",
    "        value_projected = K.dot(values, self.value_weight)\n",
    "        symbol_projected = K.dot(self.symbols, self.value_weight)\n",
    "        #symbol_projected = self.symbols\n",
    "        # Scaled dot-product attention\n",
    "        #scores = self.W /tf.math.sqrt(tf.cast(tf.shape(value_projected)[-1], tf.float32))\n",
    "        scores  = self.create_cosine_similarity_matrix(value_projected)\n",
    "        attention_output = tf.einsum('bii,bij->bij', scores, value_projected)\n",
    "        return tf.nn.swish(attention_output*symbol_projected)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "daa4992b-87b5-4e13-8086-37deb745214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rmts_transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 3, 512)]             0         []                            \n",
      "                                                                                                  \n",
      " source_embedder (TimeDistr  (None, 3, 64)                32832     ['input_7[0][0]']             \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 3, 64)                256       ['source_embedder[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " simplified_multi_head_atte  (None, 3, 1000)              64192     ['batch_normalization_19[0][0]\n",
      " ntion_19 (SimplifiedMultiH                                         ',                            \n",
      " eadAttention)                                                       'batch_normalization_19[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)        (None, 3, 1000)              0         ['simplified_multi_head_attent\n",
      "                                                                    ion_19[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_26 (Flatten)        (None, 3000)                 0         ['dropout_19[0][0]']          \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)        (None, 256)                  768256    ['flatten_26[0][0]']          \n",
      "                                                                                                  \n",
      " final_layer (Dense)         (None, 1)                    257       ['hidden_layer[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 865793 (3.30 MB)\n",
      "Trainable params: 865665 (3.30 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "D = 1000\n",
    "encoder_kwargs=dict(use_bias=True)\n",
    "ordertransformer_kwargs = dict(embedding_dim=embedding_dim, encoder_kwargs=encoder_kwargs)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, name='binary_crossentropy')\n",
    "create_opt = lambda : tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "class LARS_VSA(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, encoder_kwargs, name=None):\n",
    "        super().__init__(name=name)\n",
    "        #self.embedder = layers.Dense(embedding_dim)\n",
    "        object_embedder = tf.keras.Sequential([layers.Dense(embedding_dim)])\n",
    "        self.source_embedder = layers.TimeDistributed(object_embedder, name='source_embedder')\n",
    "        self.dropout = layers.Dropout(0.2)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.hidden_dense = layers.Dense(256, activation='relu', name='hidden_layer')\n",
    "        self.final_layer = layers.Dense(1, activation='sigmoid', name='final_layer')\n",
    "        self.mha = HDSymbolicAttention(1000) \n",
    "        self.bn = layers.BatchNormalization(synchronized=True) \n",
    "    def call(self, inputs):\n",
    "        #x = self.embedder(inputs)\n",
    "        x = self.source_embedder(inputs)\n",
    "        x = self.bn(x)\n",
    "        h0 = self.mha([x,x,x])\n",
    "        h0 = self.dropout(h0)\n",
    "        x = self.flatten(h0)\n",
    "        x = self.hidden_dense(x)\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def print_summary(self, input_shape):\n",
    "        inputs = layers.Input(input_shape)\n",
    "        outputs = self.call(inputs)\n",
    "        print(tf.keras.Model(inputs, outputs, name=self.name).summary())\n",
    "\n",
    "\n",
    "transformer_model = HDTransformerOrderModel(**ordertransformer_kwargs, name='rmts_transformer')\n",
    "transformer_model.compile(loss='binary_crossentropy', optimizer=create_opt(), metrics=['binary_accuracy'])\n",
    "transformer_model.print_summary((3,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76ef556c-683a-4e30-aa90-359ce4117f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train = np.load('train_features.npy')\n",
    "x_test = np.load('test_features.npy')\n",
    "y_train = np.load('train_labels.npy')\n",
    "y_test = np.load('test_labels.npy')\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "print(x_train.shape)\n",
    "import numpy as np\n",
    "\n",
    "def create_sequence_data(x_data, y_data, N):\n",
    "    # Initialize the new data structures\n",
    "    X_new = np.zeros((N, 3, 512))\n",
    "    Y_new = np.zeros(N)\n",
    "    \n",
    "    # Define the target patterns\n",
    "    valid_patterns = {tuple([0, 1, 0]), tuple([1, 0, 1])}\n",
    "    \n",
    "    # Loop to fill the new data structures\n",
    "    for i in range(N):\n",
    "        # Choose 3 random indices\n",
    "        indices = np.random.choice(x_data.shape[0], 3, replace=False)\n",
    "        # Extract the submatrix and corresponding labels\n",
    "        x_submatrix = x_data[indices]\n",
    "        y_sublabels = y_data[indices]\n",
    "        \n",
    "        # Check if the selected labels match the desired patterns\n",
    "        if tuple(y_sublabels) in valid_patterns:\n",
    "            Y_new[i] = 1\n",
    "        \n",
    "        # Store the submatrix in the new X matrix\n",
    "        X_new[i] = x_submatrix\n",
    "    \n",
    "    return X_new, Y_new\n",
    "\n",
    "# Example usage:\n",
    "# Assume x_train, y_train, x_val, y_val, x_test, y_test are already defined\n",
    "N_train, N_val, N_test = 10000, 5000, 5000  # Define new dataset sizes if needed\n",
    "\n",
    "# Generate new datasets\n",
    "X_train_new, Y_train_new = create_sequence_data(x_train, y_train, N_train)\n",
    "X_val_new, Y_val_new = create_sequence_data(x_val, y_val, N_val)\n",
    "X_test_new, Y_test_new = create_sequence_data(x_test, y_test, N_test)\n",
    "\n",
    "def transform_and_save(features, labels, filename_prefix):\n",
    "    np.save(filename_prefix + '_features_seq.npy', features)\n",
    "    np.save(filename_prefix + '_labels_seq.npy', labels)\n",
    "\n",
    "# Assuming you have already loaded your datasets into train_features, train_labels, etc.\n",
    "# Example usage:\n",
    "transform_and_save(X_train_new, Y_train_new, 'train')\n",
    "transform_and_save(X_val_new, Y_val_new, 'val')\n",
    "transform_and_save(X_test_new, Y_test_new, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d7a7d984-056e-499b-a053-ec24d9f1653e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m transformer_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mcreate_opt(), metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m X_train_1, y_train_1 \u001b[38;5;241m=\u001b[39m X_train[:train_size], y_train[:train_size]\n\u001b[0;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m acc \u001b[38;5;241m=\u001b[39m transformer_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m hdc\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1777\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1778\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[0;32m-> 1791\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m }\n\u001b[1;32m   1807\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2198\u001b[0m             ):\n\u001b[1;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = np.load('train_features_seq.npy')\n",
    "X_test = np.load('test_features_seq.npy')\n",
    "y_train = np.load('train_labels_seq.npy')\n",
    "y_test = np.load('test_labels_seq.npy')\n",
    "X_val = np.load('val_features_seq.npy')\n",
    "y_val = np.load('val_labels_seq.npy')\n",
    "Total_hdc = []\n",
    "for train_size in [50,100,200,500,750,1000,2000,3000,5000]:\n",
    "    hdc = []\n",
    "    for j in range(10):\n",
    "        np.random.seed(j)\n",
    "        create_opt = lambda : tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "        transformer_model = LARS_VSA(**ordertransformer_kwargs, name='rmts_transformer')\n",
    "        transformer_model.compile(loss='binary_crossentropy', optimizer=create_opt(), metrics='binary_accuracy')\n",
    "        X_train_1, y_train_1 = X_train[:train_size], y_train[:train_size]\n",
    "        history = transformer_model.fit(X_train_1, y_train_1, validation_data=(X_val, y_val), epochs=50, batch_size=512,verbose=0)\n",
    "        acc = transformer_model.evaluate(X_test, y_test,verbose=0)[1]\n",
    "        hdc.append(acc)\n",
    "    hdc = np.array(hdc)\n",
    "    Total_hdc.append(hdc)\n",
    "np.save('total_hdc.npy',Total_hdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6285f-2b4b-466a-933b-2a56633b9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_modules import Encoder, Decoder, AddPositionalEmbedding\n",
    "from abstracters import RelationalAbstracter\n",
    "from abstractor import Abstractor\n",
    "\n",
    "def create_abstractor(encoder_kwargs, abstractor_kwargs, embedding_dim, dropout_rate=0.1, name='abstractor'):\n",
    "    inputs = layers.Input(shape=[3,512], name='input_seq')\n",
    "    object_embedder = tf.keras.Sequential([layers.Dense(embedding_dim)])\n",
    "    source_embedder = layers.TimeDistributed(object_embedder, name='source_embedder')\n",
    "    # pos_embedding_adder_input = AddPositionalEmbedding(name='add_pos_embedding_input')\n",
    "    abstractor = RelationalAbstracter(**abstractor_kwargs, name='abstractor')\n",
    "    flattener = layers.Flatten()\n",
    "    hidden_dense = layers.Dense(64, activation='relu', name='hidden_dense')\n",
    "    final_layer = layers.Dense(1, name='final_layer',activation='sigmoid')\n",
    "\n",
    "    x = source_embedder(inputs)\n",
    "    # x = pos_embedding_adder_input(x)\n",
    "    abstract_states = abstractor(x)\n",
    "    x = flattener(abstract_states)\n",
    "    x = hidden_dense(x)\n",
    "    logits = final_layer(x)\n",
    "\n",
    "    abstractor_model = tf.keras.Model(inputs=inputs, outputs=logits, name=name)\n",
    "    return abstractor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a5fb2-23a4-4a88-ac9e-78660be161ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractor_kwargs = dict(num_layers=1, num_heads=4, dff=64,\n",
    "     use_pos_embedding=True, mha_activation_type='relu', dropout_rate=0.1)\n",
    "\n",
    "abstractor_model_kwargs = dict(encoder_kwargs=None, abstractor_kwargs=abstractor_kwargs, embedding_dim=64)\n",
    "abstractor_model = create_abstractor(**abstractor_model_kwargs)\n",
    "\n",
    "abstractor_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "abstractor_model(X_train[:32]); # build\n",
    "abstractor_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc8c4a-d8d5-4d2b-89ed-06f348661406",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_hdc = []\n",
    "for train_size in [50,100,200,500,750,1000,2000,3000,5000]:\n",
    "    hdc = []\n",
    "    for j in range(10):\n",
    "        np.random.seed(j)\n",
    "        create_opt = lambda : tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "        abstractor_model_kwargs = dict(encoder_kwargs=None, abstractor_kwargs=abstractor_kwargs, embedding_dim=64)\n",
    "        transformer_model = create_abstractor(**abstractor_model_kwargs)\n",
    "        transformer_model.compile(loss='binary_crossentropy', optimizer=create_opt(), metrics=['binary_accuracy'])\n",
    "        X_train_1, y_train_1 = X_train[:train_size], y_train[:train_size]\n",
    "        history = transformer_model.fit(X_train_1, y_train_1, validation_data=(X_val, y_val), epochs=50, batch_size=512,verbose=0)\n",
    "        acc = transformer_model.evaluate(X_test, y_test,verbose=0)[1]\n",
    "        hdc.append(acc)\n",
    "    hdc = np.array(hdc)\n",
    "    Total_hdc.append(hdc)\n",
    "np.save('total_abstractor.npy',Total_hdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b152e70-9673-4c62-8875-010780fd1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_modules import Encoder, Decoder, AddPositionalEmbedding\n",
    "from abstracters import SymbolicAbstracter, RelationalAbstracter\n",
    "\n",
    "def create_transformer(num_layers, num_heads, dff, embedding_dim, dropout_rate=0.1,):\n",
    "    inputs = layers.Input(shape=(3,512))\n",
    "    source_embedder = layers.TimeDistributed(layers.Dense(embedding_dim), name='source_embedder')\n",
    "    pos_embedding_adder_input = AddPositionalEmbedding(name='add_pos_embedding_input')\n",
    "    encoder = Encoder(num_layers=num_layers, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate, name='encoder')\n",
    "    flattener = layers.Flatten()\n",
    "    final_layer = layers.Dense(2, name='final_layer')\n",
    "\n",
    "    x = source_embedder(inputs)\n",
    "    x = pos_embedding_adder_input(x)\n",
    "    encoder_context = encoder(x)\n",
    "    output = flattener(encoder_context)\n",
    "    logits = final_layer(output)\n",
    "\n",
    "    transformer_model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    return transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913b1d9-8a27-4d14-b7ed-d0ec895974e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_hdc = []\n",
    "for train_size in [50,100,200,500,750,1000,2000,3000,5000]:\n",
    "    hdc = []\n",
    "    for j in range(10):\n",
    "        np.random.seed(j)\n",
    "        create_opt = lambda : tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "        transformer_model = create_transformer(num_layers=1, num_heads=4, dff=64, embedding_dim=64)\n",
    "        transformer_model.compile(loss='binary_crossentropy', optimizer=create_opt(), metrics=['binary_accuracy'])\n",
    "        X_train_1, y_train_1 = X_train[:train_size], y_train[:train_size]\n",
    "        history = transformer_model.fit(X_train_1, y_train_1, validation_data=(X_val, y_val), epochs=50, batch_size=512,verbose=1)\n",
    "        acc = transformer_model.evaluate(X_test, y_test,verbose=1)[1]\n",
    "        hdc.append(acc)\n",
    "    hdc = np.array(hdc)\n",
    "    Total_hdc.append(hdc)\n",
    "np.save('total_transformer.npy',Total_hdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aead486-93a1-429f-a969-a1f826fd3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_head_relation import MultiHeadRelation\n",
    "\n",
    "def create_corelnet(embedding_dim, activation='softmax', name='corelnet'):\n",
    "    inputs = layers.Input(shape=(3,512), name='input_seq')\n",
    "    object_embedder = tf.keras.Sequential([layers.Dense(embedding_dim)])\n",
    "    source_embedder = layers.TimeDistributed(object_embedder, name='source_embedder')\n",
    "    activation = layers.Softmax(axis=1) if activation == 'softmax' else layers.Activation(activation)\n",
    "    mhr = MultiHeadRelation(rel_dim=1, proj_dim=None, symmetric=True, dense_kwargs=dict(use_bias=False))\n",
    "    flattener = layers.Flatten()\n",
    "    final_layer = layers.Dense(1, name='final_layer',activation='sigmoid')\n",
    "\n",
    "    x = source_embedder(inputs)\n",
    "    x = mhr(x)\n",
    "    x = activation(x)\n",
    "    x = flattener(x)\n",
    "    logits = final_layer(x)\n",
    "\n",
    "    corelnet_model = tf.keras.Model(inputs=inputs, outputs=logits, name=name)\n",
    "    return corelnet_model\n",
    "    \n",
    "Total_hdc = []\n",
    "for train_size in [50,100,200,500,750,1000,2000,3000,5000]:\n",
    "    hdc = []\n",
    "    for j in range(10):\n",
    "        np.random.seed(j)\n",
    "        create_opt = lambda : tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "        abstractor_model_kwargs = dict(encoder_kwargs=None, abstractor_kwargs=abstractor_kwargs, embedding_dim=64)\n",
    "        transformer_model = create_corelnet(embedding_dim=64)\n",
    "        transformer_model.compile(loss='binary_crossentropy', optimizer=create_opt(), metrics=['binary_accuracy'])\n",
    "        X_train_1, y_train_1 = X_train[:train_size], y_train[:train_size]\n",
    "        history = transformer_model.fit(X_train_1, y_train_1, validation_data=(X_val, y_val), epochs=50, batch_size=512,verbose=0)\n",
    "        acc = transformer_model.evaluate(X_test, y_test,verbose=0)[1]\n",
    "        hdc.append(acc)\n",
    "    hdc = np.array(hdc)\n",
    "    Total_hdc.append(hdc)\n",
    "np.save('total_corelnet.npy',Total_hdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d9d40-2a96-42d5-9934-38c5c02c8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_head_relation import MultiHeadRelation\n",
    "\n",
    "def create_corelnet(embedding_dim, activation='relu', name='corelnet'):\n",
    "    inputs = layers.Input(shape=(3,512), name='input_seq')\n",
    "    object_embedder = tf.keras.Sequential([layers.Dense(embedding_dim)])\n",
    "    source_embedder = layers.TimeDistributed(object_embedder, name='source_embedder')\n",
    "    activation = layers.Softmax(axis=1) if activation == 'softmax' else layers.Activation(activation)\n",
    "    mhr = MultiHeadRelation(rel_dim=1, proj_dim=None, symmetric=True, dense_kwargs=dict(use_bias=False))\n",
    "    flattener = layers.Flatten()\n",
    "    final_layer = layers.Dense(1, name='final_layer',activation='sigmoid')\n",
    "\n",
    "    x = source_embedder(inputs)\n",
    "    x = mhr(x)\n",
    "    x = activation(x)\n",
    "    x = flattener(x)\n",
    "    logits = final_layer(x)\n",
    "\n",
    "    corelnet_model = tf.keras.Model(inputs=inputs, outputs=logits, name=name)\n",
    "    return corelnet_model\n",
    "    \n",
    "Total_hdc = []\n",
    "for train_size in [50,100,200,500,750,1000,2000,3000,5000]:\n",
    "    hdc = []\n",
    "    for j in range(10):\n",
    "        np.random.seed(j)\n",
    "        create_opt = lambda : tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "        abstractor_model_kwargs = dict(encoder_kwargs=None, abstractor_kwargs=abstractor_kwargs, embedding_dim=64)\n",
    "        transformer_model = create_corelnet(embedding_dim=64)\n",
    "        transformer_model.compile(loss='binary_crossentropy', optimizer=create_opt(), metrics=['binary_accuracy'])\n",
    "        X_train_1, y_train_1 = X_train[:train_size], y_train[:train_size]\n",
    "        history = transformer_model.fit(X_train_1, y_train_1, validation_data=(X_val, y_val), epochs=50, batch_size=512,verbose=0)\n",
    "        acc = transformer_model.evaluate(X_test, y_test,verbose=0)[1]\n",
    "        hdc.append(acc)\n",
    "    hdc = np.array(hdc)\n",
    "    Total_hdc.append(hdc)\n",
    "np.save('total_corelnet_relu.npy',Total_hdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae8161-bcf6-4597-99e1-db3f02f03397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from baseline_models.predinet import PrediNet\n",
    "embedding_dim = 64\n",
    "predinet_kwargs = dict(embedding_dim=embedding_dim, predinet_kwargs=dict(key_dim=4, n_heads=4, n_relations=16, add_temp_tag=False))\n",
    "\n",
    "def create_predinet(embedding_dim, predinet_kwargs, name=None):\n",
    "    predinet_model = tf.keras.Sequential([\n",
    "        PrediNet(**predinet_kwargs),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, name='final_layer',activation='sigmoid')\n",
    "    ], name=name)\n",
    "    return predinet_model\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, name='binary_crossentropy')\n",
    "create_opt = lambda : tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "Total_hdc = []\n",
    "for train_size in [50,100,200,500,750,1000,2000,3000,5000]:\n",
    "    hdc = []\n",
    "    for j in range(10):\n",
    "        np.random.seed(j)\n",
    "        create_opt = lambda : tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        transformer_model = create_predinet(**predinet_kwargs, name='predinet')\n",
    "        transformer_model.compile(loss='binary_crossentropy', optimizer=create_opt(), metrics=['binary_accuracy'])\n",
    "        X_train_1, y_train_1 = X_train[:train_size], y_train[:train_size]\n",
    "        history = transformer_model.fit(X_train_1, y_train_1, validation_data=(X_val, y_val), epochs=50, batch_size=512,verbose=0)\n",
    "        acc = transformer_model.evaluate(X_test, y_test,verbose=1)[1]\n",
    "        hdc.append(acc)\n",
    "    hdc = np.array(hdc)\n",
    "    Total_hdc.append(hdc)\n",
    "np.save('total_predinet.npy',Total_hdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736108cd-778f-45d6-b766-8245ae2a1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from baseline_models.predinet import PrediNet\n",
    "embedding_dim = 64\n",
    "predinet_kwargs = dict(embedding_dim=embedding_dim, predinet_kwargs=dict(key_dim=4, n_heads=4, n_relations=16, add_temp_tag=False))\n",
    "\n",
    "def create_mlp(embedding_dim, predinet_kwargs, name=None):\n",
    "    predinet_model = tf.keras.Sequential([\n",
    "        layers.Dense(64, name='hidden_layer',activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, name='final_layer',activation='sigmoid')\n",
    "    ], name=name)\n",
    "    return predinet_model\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, name='binary_crossentropy')\n",
    "create_opt = lambda : tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "Total_hdc = []\n",
    "for train_size in [50,100,200,500,750,1000,2000,3000,5000]:\n",
    "    hdc = []\n",
    "    for j in range(10):\n",
    "        np.random.seed(j)\n",
    "        transformer_model = create_mlp(**predinet_kwargs, name='predinet')\n",
    "        transformer_model.compile(loss='binary_crossentropy', optimizer=create_opt(), metrics=['binary_accuracy'])\n",
    "        X_train_1, y_train_1 = X_train[:train_size], y_train[:train_size]\n",
    "        history = transformer_model.fit(X_train_1, y_train_1, validation_data=(X_val, y_val), epochs=50, batch_size=512,verbose=0)\n",
    "        acc = transformer_model.evaluate(X_test, y_test,verbose=0)[1]\n",
    "        hdc.append(acc)\n",
    "    hdc = np.array(hdc)\n",
    "    Total_hdc.append(hdc)\n",
    "np.save('total_mlp.npy',Total_hdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5784f-5e59-41f8-80a0-976e8ecf7de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
